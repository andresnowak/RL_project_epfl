{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8676f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformReward\n",
    "from stable_baselines3 import DDPG, PPO\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create directories for logs and models\n",
    "log_dir = \"./ddpg_logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c4b02",
   "metadata": {},
   "source": [
    "### Energy-Based Reward (Often Considered Best):\n",
    "\n",
    "This is theoretically well-grounded and directly encourages the physics needed to solve the task. The agent needs to build potential energy (height) and kinetic energy (velocity).\n",
    "\n",
    "- Concept: Reward the agent based on its mechanical energy (potential + kinetic). Potential energy is proportional to height (sin(3 * position) in this environment's dynamics), and kinetic energy is proportional to velocity^2.\n",
    "- Potential Function (phi(state)): Define a potential function based on energy. A common form is:\n",
    "phi(position, velocity) = C1 * sin(3 * position) + C2 * velocity**2\n",
    "where C1 and C2 are positive scaling constants you need to tune (e.g., C1=1, C2=1 or C1=10, C2=1).\n",
    "- Shaped Reward (r_shaped): Use potential-based reward shaping. The reward at a step is the original reward plus the change in potential, discounted by gamma (the RL algorithm's discount factor).\n",
    "r_shaped = r_original + gamma * phi(next_position, next_velocity) - phi(position, velocity)\n",
    "- Implementation:\n",
    "Remove the original action penalty (r_original = 0 except for the goal).\n",
    "Keep the +100 goal reward.\n",
    "reward = (100 if goal_reached else 0) + gamma * phi(next_state) - phi(current_state)\n",
    "\n",
    "\n",
    "**Why it's good: Directly rewards increasing energy, which is precisely the strategy needed to climb the hill. It provides dense feedback on every step based on progress in energy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43f5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarContinuousRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, gamma=0.99):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "        self.prev_pos = None\n",
    "        self.prev_vel = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        self.prev_pos = observation[0]\n",
    "        self.prev_vel = observation[1]\n",
    "        return observation, info\n",
    "\n",
    "    def potential(self, position, velocity):\n",
    "        # Tunable constants\n",
    "        C1 = 10.0  # Weight for potential energy (height)\n",
    "        C2 = 1.0   # Weight for kinetic energy (velocity^2)\n",
    "        # Height approximation based on environment dynamics\n",
    "        height = np.sin(3 * position)\n",
    "        # Kinetic energy approximation\n",
    "        kinetic_energy = 0.5 * velocity**2 # Mass implicitly handled by scaling C2\n",
    "        return C1 * height + C2 * kinetic_energy\n",
    "\n",
    "    def reward(self, reward, obs):\n",
    "        # Note: `reward` argument is the original reward from the underlying env\n",
    "        # We will mostly ignore it, except for the +100 goal bonus.\n",
    "\n",
    "        position, velocity = obs\n",
    "        goal_reached = position >= 0.45\n",
    "\n",
    "        # Original reward (only the goal part matters)\n",
    "        original_reward = 100.0 if goal_reached else 0.0\n",
    "\n",
    "        # Calculate potentials for shaping\n",
    "        prev_potential = self.potential(self.prev_pos, self.prev_vel)\n",
    "        current_potential = self.potential(position, velocity)\n",
    "\n",
    "        # Update history for next step\n",
    "        self.prev_pos = position\n",
    "        self.prev_vel = velocity\n",
    "\n",
    "        # Calculate shaped reward\n",
    "        shaped_reward = original_reward + self.gamma * current_potential - prev_potential\n",
    "\n",
    "        # We removed the default action penalty implicitly by calculating from potential\n",
    "        return shaped_reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "        info[\"original_reward\"] = reward  # Store original reward\n",
    "\n",
    "        reward = self.reward(reward, obs)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2682a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746f412f4cff4d8cbedac401fbed73db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a vectorized environment (optional: use `make_vec_env` for parallelism)\n",
    "# env = gym.make_vec(\"MountainCarContinuous-v0\", num_envs=4)  # Classic continuous control task\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "def shaped_reward(reward):\n",
    "    position = env.unwrapped.state[0]\n",
    "    velocity = env.unwrapped.state[1]\n",
    "    base_shaping = position + 0.1 * velocity\n",
    "    elapsed = env.unwrapped._elapsed_steps\n",
    "\n",
    "    goal_bonus = 0.0\n",
    "    if position >= 0.45:\n",
    "        goal_bonus = 100.0  # Manual reward for reaching the goal\n",
    "\n",
    "    return base_shaping + reward + goal_bonus\n",
    "\n",
    "# env = TransformReward(env, shaped_reward)\n",
    "env = MountainCarContinuousRewardWrapper(env)\n",
    "env = Monitor(env, log_dir)  # Wrap for logging\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    device=\"auto\",\n",
    "    tensorboard_log=\"./ppo_tensorboard/\",\n",
    "    gamma=0.99,\n",
    "    learning_rate=3e-4,  # PPO's default LR\n",
    "    n_steps=2048,  # Steps per environment per update\n",
    "    batch_size=64,\n",
    "    n_epochs=10,  # Number of optimization epochs per update\n",
    "    clip_range=0.2,  # PPO's clipping parameter\n",
    "    ent_coef=0.01,     # Encourages exploration\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=200_000, progress_bar=True)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_mountaincar\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 93.25 ± 0.62\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent (with the original reward)\n",
    "model = PPO.load(\"ppo_mountaincar\")\n",
    "eval_env = Monitor(gym.make(\"MountainCarContinuous-v0\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "\n",
    "# Close environments\n",
    "eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648595f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:28:03.715 python[97076:14017939] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-27 19:28:03.715 python[97076:14017939] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "# Visualize the trained agent\n",
    "def visualize_agent(model, env, episodes=5):\n",
    "    try:\n",
    "        for ep in range(episodes):\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                try:\n",
    "                    env.render()\n",
    "                except Exception as e:\n",
    "                    print(f\"Render failed (probably closed window): {e}\")\n",
    "                    return  # Exit the visualization early\n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "# Create a new environment for visualization (with rendering)\n",
    "vis_env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "visualize_agent(model, vis_env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Example ---\n",
      "Model loaded from ppo_mountaincar.zip\n",
      "\n",
      "Calling generate_and_save_rollouts...\n",
      "\n",
      "Collecting data from partial model...\n",
      "Model 'partial' - Episode 1/5: Return=93.49, Length=77\n",
      "Model 'partial' - Episode 2/5: Return=93.67, Length=85\n",
      "Model 'partial' - Episode 3/5: Return=93.02, Length=79\n",
      "Model 'partial' - Episode 4/5: Return=93.54, Length=79\n",
      "Model 'partial' - Episode 5/5: Return=91.54, Length=113\n",
      "Saved trajectories for partial model to ppo_mountaincar_continuous_rollouts/partial_model_trajectories.csv\n",
      "\n",
      "Collecting data from full model...\n",
      "Model 'full' - Episode 1/5: Return=93.12, Length=78\n",
      "Model 'full' - Episode 2/5: Return=93.26, Length=77\n",
      "Model 'full' - Episode 3/5: Return=91.36, Length=110\n",
      "Model 'full' - Episode 4/5: Return=93.02, Length=79\n",
      "Model 'full' - Episode 5/5: Return=93.49, Length=77\n",
      "Saved trajectories for full model to ppo_mountaincar_continuous_rollouts/full_model_trajectories.csv\n",
      "\n",
      "Created 35 preference pairs based on trajectory returns.\n",
      "Preference pairs saved to ppo_mountaincar_continuous_rollouts/preference_pairs.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from src.generate_demonstrations import collect_paired_demonstrations\n",
    "\n",
    "print(\"--- Running Example ---\")\n",
    "\n",
    "# Define parameters based on the user request\n",
    "MODEL_PATH = \"ppo_mountaincar.zip\" # Use Continuous version\n",
    "ENV_ID = \"MountainCarContinuous-v0\"\n",
    "CSV_FILE = \"ppo_mountaincar_continuous_rollouts.csv\"\n",
    "DIR_NAME = \"ppo_mountaincar_continuous_rollouts\"\n",
    "NUM_EPISODES = 5\n",
    "DETERMINISTIC_ROLLOUT = False # Use stochastic actions for variety\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "model = PPO.load(MODEL_PATH, device=device)\n",
    "model_2 = PPO.load(MODEL_PATH, device=device)\n",
    "\n",
    "print(f\"Model loaded from {MODEL_PATH}\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(ENV_ID)\n",
    "# NOTE: we use the original reward\n",
    "\n",
    "print(\"\\nCalling generate_and_save_rollouts...\")\n",
    "\n",
    "collect_paired_demonstrations(\n",
    "    model_2,\n",
    "    model,\n",
    "    env,\n",
    "    DIR_NAME,\n",
    "    5,\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a6167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
